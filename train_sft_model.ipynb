{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import traceback\n",
    "from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "from accelerate.utils import pad_across_processes, broadcast\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, Dataset, concatenate_datasets\n",
    "from datetime import datetime,  timedelta\n",
    "import time\n",
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from src.python_engine import run_python_code\n",
    "from src.utils import set_seed, floatify, compute_ETA\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup, AdamW, get_constant_schedule_with_warmup\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from accelerate import notebook_launcher\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tqdm = partial(tqdm, ncols=0, leave=False)\n",
    "\n",
    "\n",
    "TIMEOUT = 10\n",
    "instruction=None\n",
    "cot_trigger=None\n",
    "answer_trigger=None\n",
    "def setup_cot(src_name):\n",
    "    assert src_name in ['gsm8k', 'mathqa', 'svamp', 'mathqa-numeric', 'zhouyi']\n",
    "    global instruction\n",
    "    global cot_trigger\n",
    "    global answer_trigger\n",
    "    # Complete output is in this form: f'{instruction}{question.strip()}{cot_trigger}{answer_cot.strip()}'\n",
    "    instruction = 'Question:\\n'\n",
    "    cot_trigger = '\\nAnswer reasoning:\\n'\n",
    "    answer_trigger = '\\n因此，答案是：'\n",
    "    return \n",
    "\n",
    "post_process_final_answer_fn_mapper = {\n",
    "    'gsm8k': lambda x: float(x.replace(',','').strip()),\n",
    "    'svamp': lambda x: float(x.replace(',','').strip()),\n",
    "    'mathqa': lambda x: x.lower().replace('\"','').replace(\"'\",'').strip(),\n",
    "    'mathqa-numeric': lambda x: float(x),\n",
    "    'zhouyi': lambda x: x.strip(),\n",
    "}\n",
    "### the answer_cot is a list of answer_cot\n",
    "post_process_answer_cot_fn_mapper = {\n",
    "    ('python', 'gsm8k'): lambda answer_cot: [floatify(res) for res in run_python_code(programs=answer_cot, TIMEOUT=TIMEOUT)],\n",
    "    ('python', 'svamp'): lambda answer_cot: [floatify(res) for res in run_python_code(programs=answer_cot, TIMEOUT=TIMEOUT)],\n",
    "    ('python', 'mathqa'): lambda answer_cot: [str(res).lower().replace('\"','').replace(\"'\",'').strip() for res in run_python_code(programs=answer_cot, TIMEOUT=TIMEOUT)],\n",
    "    ('python', 'mathqa-numeric'): lambda answer_cot: [floatify(res) for res in run_python_code(programs=answer_cot, TIMEOUT=TIMEOUT)],\n",
    "    ('nl', 'gsm8k'): lambda answer_cot: [floatify(res.split(answer_trigger)[-1].strip()) for res in answer_cot],\n",
    "    ('nl', 'svamp'): lambda answer_cot: [floatify(res.split(answer_trigger)[-1].strip()) for res in answer_cot],\n",
    "    ('nl', 'mathqa'): lambda answer_cot: [res.split(answer_trigger)[-1].lower().replace('\"','').replace(\"'\",'').strip() for res in answer_cot],\n",
    "    ('nl', 'mathqa-numeric'): lambda answer_cot: [floatify(res.split(answer_trigger)[-1].strip()) for res in answer_cot],\n",
    "    ('nl', 'zhouyi'): lambda answer_cot: [res.split(answer_trigger)[-1].strip() for res in answer_cot],\n",
    "}\n",
    "compare_answer_fn_mapper = {\n",
    "    'gsm8k': lambda extracted_ans, target_answer: abs(extracted_ans - target_answer) <= 1e-2,\n",
    "    'svamp': lambda extracted_ans, target_answer: abs(extracted_ans - target_answer) <= 1e-2,\n",
    "    'mathqa': lambda extracted_ans, target_answer: extracted_ans == target_answer,\n",
    "    'mathqa-numeric': lambda extracted_ans, target_answer: abs(extracted_ans - target_answer) <= 1e-2,\n",
    "    'zhouyi': lambda extracted_ans, target_answer: extracted_ans == target_answer,\n",
    "}\n",
    "\n",
    "def prepare_datasets_and_data_loaders(args, tokenizer):\n",
    "    # 确保所有进程同步开始准备数据\n",
    "    # accelerator.wait_for_everyone()\n",
    "    \n",
    "    with accelerator.main_process_first():\n",
    "        raw_dataset = DatasetDict({\n",
    "            'train': Dataset.from_list(json.load(open(args['train_file'],'r'))),\n",
    "            'test': Dataset.from_list(json.load(open(args['test_file'],'r'))),\n",
    "        })\n",
    "        accelerator.print('Raw data:', raw_dataset)\n",
    "        src_name = raw_dataset['train'][0]['item_id'].split('_')[0]  # e.g., gsm8k_0, gsm8k_1, gsm8k_2, ...\n",
    "        setup_cot(src_name)\n",
    "        accelerator.print('Using instruction:', instruction)\n",
    "        accelerator.print('Using cot_trigger:', cot_trigger)\n",
    "        accelerator.print('Using answer_trigger:', answer_trigger)\n",
    "        def tokenize_fn(batch, args, tokenizer):\n",
    "            assert tokenizer.eos_token_id is not None, (tokenizer.eos_token_id, tokenizer.eos_token)\n",
    "            new_batch = defaultdict(list)\n",
    "            all_keys = list(batch.keys())\n",
    "            for item_values in zip(*(batch[k] for k in all_keys)):\n",
    "                item = {k: item_values[i] for i, k in enumerate(all_keys)}\n",
    "                item_id, question, answer_value, answer_cot = \\\n",
    "                        item['item_id'], \\\n",
    "                        item['question'], \\\n",
    "                        item['answer_value'], \\\n",
    "                        item.get('answer_cot', None), \\\n",
    "\n",
    "                question = question.strip()\n",
    "                if answer_value is not None:\n",
    "                    answer_value = answer_value.strip()\n",
    "\n",
    "                if answer_cot is not None:\n",
    "                    answer_cot = answer_cot.strip()\n",
    "                    if args['engine'] == 'nl':\n",
    "                        answer_cot += f'{answer_trigger}{answer_value}'\n",
    "\n",
    "                input = f'{instruction}{question}{cot_trigger}'\n",
    "                output = f'{answer_cot}'\n",
    "                prefix_text = f'{instruction}{question}{cot_trigger}'\n",
    "\n",
    "                input_encode = tokenizer(input, add_special_tokens=False)\n",
    "                output_encode = tokenizer(output, add_special_tokens=False)\n",
    "                prefix_encode = tokenizer(prefix_text, add_special_tokens=False)\n",
    "\n",
    "                input_ids = input_encode['input_ids'] + output_encode['input_ids'] + [tokenizer.eos_token_id]\n",
    "                labels = [-100]*len(input_encode['input_ids']) + output_encode['input_ids'] + [tokenizer.eos_token_id]\n",
    "                attention_mask = [1]* len(input_ids)\n",
    "                prefix = prefix_encode['input_ids']\n",
    "                prefix_attention_mask = prefix_encode['attention_mask']\n",
    "\n",
    "                # Truncation\n",
    "                input_ids_max_length = len(input_ids)\n",
    "                # assert input_ids_max_length <= args['max_input_length'], input_ids_max_length\n",
    "                input_ids = input_ids[:args['max_input_length']]\n",
    "                labels = labels[:args['max_input_length']]\n",
    "                attention_mask = attention_mask[:args['max_input_length']]\n",
    "                prefix = prefix[:args['max_input_length']]\n",
    "                prefix_attention_mask = prefix_attention_mask[:args['max_input_length']]\n",
    "\n",
    "                ##\n",
    "                new_batch['input_ids'].append(input_ids)\n",
    "                new_batch['labels'].append(labels)\n",
    "                new_batch['attention_mask'].append(attention_mask)\n",
    "                new_batch['prefix'].append(prefix)\n",
    "                new_batch['prefix_attention_mask'].append(prefix_attention_mask)\n",
    "                ##\n",
    "                new_batch['item_id'].append(item_id)\n",
    "                new_batch['question'].append(question)\n",
    "                new_batch['answer_cot'].append(answer_cot)\n",
    "                new_batch['answer_value'].append(answer_value)\n",
    "                new_batch['input_ids_max_length'].append(input_ids_max_length)\n",
    "            \n",
    "            return new_batch\n",
    "\n",
    "        tokenized_dataset = DatasetDict({\n",
    "            mode: dataset.map(\n",
    "                tokenize_fn, fn_kwargs={'args': args, 'tokenizer': tokenizer}, batched=True, remove_columns=dataset.column_names, \n",
    "                num_proc=8, load_from_cache_file=False\n",
    "            ) for mode, dataset in raw_dataset.items()})\n",
    "        accelerator.print('Processed data:', tokenized_dataset)\n",
    "        for mode, dataset in tokenized_dataset.items():\n",
    "            accelerator.print(mode, f'{mode}_input_ids_max_length', max(dataset['input_ids_max_length']))\n",
    "\n",
    "        if accelerator.is_main_process and args['wandb_log']:\n",
    "            wandb.config.update({\n",
    "                \"src_name\": src_name,\n",
    "                \"instruction\": instruction,\n",
    "                \"cot_trigger\": cot_trigger,\n",
    "                \"answer_trigger\": answer_trigger,\n",
    "                \"raw_dataset\": str(raw_dataset),\n",
    "                \"tokenized_dataset\": str(tokenized_dataset),\n",
    "                \"train_input_ids_max_length\": max(tokenized_dataset['train']['input_ids_max_length']),\n",
    "                \"test_input_ids_max_length\": max(tokenized_dataset['test']['input_ids_max_length']),\n",
    "            })\n",
    "\n",
    "    def collate_fn(batch, args, tokenizer):\n",
    "        max_input_length = max([len(item['input_ids']) for item in batch])\n",
    "        max_target_length = max([len(item['labels']) for item in batch])\n",
    "        max_prefix_length = max([len(item['prefix']) for item in batch])\n",
    "        input_ids  = []\n",
    "        attention_mask  = []\n",
    "        labels, labels_left_padded  = [], []\n",
    "        prefix_left_padded  = []\n",
    "        prefix_attention_mask_left_padded  = []\n",
    "        for item in batch:\n",
    "            input_ids.append(item['input_ids'] + [tokenizer.pad_token_id]*(max_input_length - len(item['input_ids'])))\n",
    "            attention_mask.append(item['attention_mask'] + [0]*(max_input_length - len(item['attention_mask'])))\n",
    "            labels.append(item['labels'] + [-100]*(max_target_length - len(item['labels'])))\n",
    "\n",
    "            labels_left_padded.append([-100]*(max_target_length - len(item['labels'])) + item['labels'])\n",
    "            prefix_left_padded.append([tokenizer.pad_token_id]*(max_prefix_length - len(item['prefix'])) + item['prefix'])\n",
    "            prefix_attention_mask_left_padded.append([0]*(max_prefix_length - len(item['prefix_attention_mask'])) + item['prefix_attention_mask'])\n",
    "        forward_kwargs = {\n",
    "            'input_ids': torch.LongTensor(input_ids),\n",
    "            'attention_mask': torch.BoolTensor(attention_mask),\n",
    "            'labels': torch.LongTensor(labels)\n",
    "        }\n",
    "        generate_prefix_kwargs = {\n",
    "            'input_ids': torch.LongTensor(prefix_left_padded),\n",
    "            'attention_mask': torch.BoolTensor(prefix_attention_mask_left_padded),\n",
    "            'labels': torch.LongTensor(labels_left_padded)\n",
    "        }\n",
    "        return {\n",
    "            'forward_kwargs': forward_kwargs,\n",
    "            'generate_prefix_kwargs': generate_prefix_kwargs,\n",
    "        }\n",
    "    # 1. 创建分布式采样器\n",
    "    train_sampler = DistributedSampler(\n",
    "        tokenized_dataset['train'],\n",
    "        num_replicas=accelerator.num_processes,\n",
    "        rank=accelerator.process_index,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_dataset['train'], \n",
    "        # sampler=train_sampler,  # 使用分布式采样器替代shuffle\n",
    "        batch_size=args['batch_size'], \n",
    "        num_workers=args['num_workers'], \n",
    "        pin_memory=True, \n",
    "        collate_fn=partial(collate_fn, args=args, tokenizer=tokenizer),\n",
    "        drop_last=True\n",
    "    )\n",
    "                        \n",
    "    test_dataloader = DataLoader(tokenized_dataset['test'], shuffle=False, batch_size=args['eval_batch_size'], num_workers=args['num_workers'], pin_memory=True, \n",
    "                        collate_fn=partial(collate_fn, args=args, tokenizer=tokenizer))\n",
    "    \n",
    "    # accelerator.wait_for_everyone()                    \n",
    "    return (tokenized_dataset['train'], train_dataloader), (tokenized_dataset['test'], test_dataloader)\n",
    "\n",
    "def do_checkpoint(args, model, tokenizer, save_path, global_step):\n",
    "    try:\n",
    "        # 确保进程按顺序打印\n",
    "        for rank in range(accelerator.num_processes):\n",
    "            if accelerator.process_index == rank:\n",
    "                accelerator.print(f\"\\n{'='*50}\")\n",
    "                accelerator.print(f\"[进程 {rank} 的参数信息]\")\n",
    "                accelerator.print(f\"{'='*50}\\n\")\n",
    "                \n",
    "                # 1. 打印总参数量\n",
    "                total_params = sum(p.numel() for p in model.parameters())\n",
    "                accelerator.print(f\"总参数量: {total_params}\")\n",
    "                                    \n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"[进程 {accelerator.process_index}] 打印参数信息失败: {e}\")\n",
    "        return False\n",
    "            \n",
    "    try:\n",
    "        accelerator.wait_for_everyone()\n",
    "        accelerator.print(f\"Rank {accelerator.process_index} do_checkpoint 同步成功\")\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Rank {accelerator.process_index} do_checkpoint 同步失败: {e}\")\n",
    "        return False\n",
    "    # 保存config tokenizer \n",
    "    try:\n",
    "        if accelerator.is_main_process:\n",
    "            config = AutoConfig.from_pretrained(args[\"model_name_or_path\"], trust_remote_code=True)\n",
    "            config.save_pretrained(args[\"model_dir\"])\n",
    "            tokenizer.save_pretrained(args[\"model_dir\"])\n",
    "    except:\n",
    "        accelerator.print(f\"Rank {accelerator.process_index} do_checkpoint config tokenizer 保存失败\")\n",
    "        return False\n",
    "    try:\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "        # 打印state_dict\n",
    "        accelerator.print(f\"{'='*50} unwrapped_model.named_parameters()\")\n",
    "        for name, param in unwrapped_model.named_parameters():\n",
    "            accelerator.print(f\"参数 {name}: shape {param.shape}\")\n",
    "        accelerator.print(f\"{'='*50} model.named_parameters()\")\n",
    "        for name, param in model.named_parameters():\n",
    "            accelerator.print(f\"参数 {name}: shape {param.shape}\")\n",
    "        accelerator.print(f\"{'='*50}\")\n",
    "        \n",
    "        accelerator.print(f\"Rank {accelerator.process_index} do_checkpoint state_dict 收集成功\")\n",
    "    except:\n",
    "        accelerator.print(f\"Rank {accelerator.process_index} do_checkpoint state_dict 收集失败\")\n",
    "        return False\n",
    "    try:        \n",
    "        # if accelerator.is_main_process:\n",
    "        unwrapped_model.save_pretrained(\n",
    "            save_path,\n",
    "            is_main_process=accelerator.is_main_process,\n",
    "            save_function=accelerator.save,\n",
    "            # 且分为n个模型文件\n",
    "            max_shard_size=\"2GB\",  # 添加此参数来控制分片大小\n",
    "            safe_serialization=True,  # 使用安全的序列化方式\n",
    "            state_dict = accelerator.get_state_dict(model)\n",
    "            )\n",
    "        accelerator.print('save checkpoint success!')\n",
    "        accelerator.wait_for_everyone()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Rank {accelerator.process_index} 保存失败: {e}\")\n",
    "        # accelerator.wait_for_everyone()\n",
    "        return False\n",
    "\n",
    "def train_one_epoch(args, model, train_dataset, train_dataloader, optimizer, scheduler, tokenizer,\n",
    "                    global_step, test_dataset, test_dataloader, \n",
    "                    prefix, epoch, best_eval_log_dict, summary_log_dict, most_recent_ckpts_paths):\n",
    "    \n",
    "    model_dir = args['model_dir']\n",
    "    clip_grad_norm = args.get('clip_grad_norm', None)\n",
    "    evaluating_step_freq = args.get('evaluating_step_freq', None)\n",
    "    logging_step_freq = args.get('logging_step_freq', None)\n",
    "    saving_step_freq = args.get('saving_step_freq', None)\n",
    "    model.train()\n",
    "    epoch_result_dict = defaultdict(list)\n",
    "    gradient_accumulation_steps = args['gradient_accumulation_steps']\n",
    "    optimizer.zero_grad()\n",
    "    accelerator.print(f\"Rank {accelerator.process_index} 进入 train_one_epoch\")\n",
    "    \n",
    "    # 打印train_dataloader\n",
    "    accelerator.print(f\"Rank {accelerator.process_index} train_dataloader: {train_dataloader}\")\n",
    "    # 确保在开始循环前同步RNG状态\n",
    "    try:\n",
    "        accelerator.wait_for_everyone()\n",
    "        accelerator.print(f\"Rank {accelerator.process_index} train_one_epoch 同步成功\")\n",
    "    except Exception as e:\n",
    "        accelerator.print(f\"Rank {accelerator.process_index} train_one_epoch 同步失败: {e}\")\n",
    "        return False\n",
    "    pbar = tqdm(total=len(train_dataloader), desc=f'Train Loop [{accelerator.process_index}]')\n",
    "    \n",
    "    # with tqdm(enumerate(train_dataloader), total=len(train_dataloader), disable=not accelerator.is_main_process, desc='Train Loop') as t:\n",
    "    for idx, batch in  enumerate(train_dataloader):\n",
    "        try:\n",
    "            accelerator.wait_for_everyone()\n",
    "            accelerator.print(f\"Rank {accelerator.process_index} train_one_epoch 第{idx}批次同步成功\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"Rank {accelerator.process_index} train_one_epoch 第{idx}批次同步失败: {e}\")\n",
    "            try:\n",
    "                accelerator.wait_for_everyone()\n",
    "                accelerator.print(f\"Rank {accelerator.process_index} train_one_epoch 第{idx}批次重新同步成功\")\n",
    "            except Exception as e:\n",
    "                accelerator.print(f\"Rank {accelerator.process_index} train_one_epoch 第{idx}批次重新同步失败: {e}\")\n",
    "        try:\n",
    "            # 前向传播\n",
    "            output = model(**batch['forward_kwargs'])\n",
    "            loss = output[0]\n",
    "            \n",
    "            # 缩放损失以适应梯度累积\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            \n",
    "            # 反向传播\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            # 记录指标\n",
    "            result_dict, extra = {}, None\n",
    "            \n",
    "            # 在累积足够步数后更新\n",
    "            if (idx + 1) % gradient_accumulation_steps == 0:\n",
    "                if clip_grad_norm is not None:\n",
    "                    accelerator.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 更新全局步数\n",
    "                global_step += 1\n",
    "                \n",
    "                # 记录训练损失\n",
    "                epoch_result_dict['loss'].append(loss.item() * gradient_accumulation_steps)\n",
    "                for k, v in result_dict.items():\n",
    "                    epoch_result_dict[k].append(v)\n",
    "\n",
    "                # 评估逻辑\n",
    "                eval_log_dict = {}\n",
    "                is_best = False\n",
    "                if evaluating_step_freq is not None and global_step % evaluating_step_freq == 0:\n",
    "                    evaluate_result_dict = {\n",
    "                        f'Eval.Gen.{k}': v \n",
    "                        for k, v in evaluate_generation(args, model, test_dataset, test_dataloader, tokenizer).items()\n",
    "                    }\n",
    "                    eval_log_dict.update(evaluate_result_dict)\n",
    "                    if eval_log_dict['Eval.Gen.value_accuracy'] > best_eval_log_dict.get('Eval.Gen.value_accuracy_best', 0):\n",
    "                        is_best = True\n",
    "                        best_eval_log_dict['Eval.Gen.value_accuracy_best'] = eval_log_dict['Eval.Gen.value_accuracy']\n",
    "                    if 'Eval.Gen.value_accuracy' not in summary_log_dict:\n",
    "                        summary_log_dict['Eval.Gen.value_accuracy'] = []\n",
    "                    summary_log_dict['Eval.Gen.value_accuracy'].append(eval_log_dict['Eval.Gen.value_accuracy'])\n",
    "\n",
    "                # 日志记录\n",
    "                train_log_dict = {}\n",
    "                if logging_step_freq is not None and global_step % logging_step_freq == 0:\n",
    "                    train_log_dict = {\n",
    "                        f'T.{k}': sum(v)/len(v) if isinstance(v, list) else v \n",
    "                        for k, v in epoch_result_dict.items()\n",
    "                    }\n",
    "                \n",
    "                if eval_log_dict or train_log_dict:\n",
    "                    log_dict = {\n",
    "                        'lr': scheduler.get_last_lr()[0], \n",
    "                        **train_log_dict, \n",
    "                        **eval_log_dict, \n",
    "                        **best_eval_log_dict\n",
    "                    }\n",
    "                    if accelerator.is_main_process and args['wandb_log']:\n",
    "                        wandb.log(log_dict, step=global_step)\n",
    "                        log_dict = {'wandb': args['wandb_project'] + '|' + args['wandb_run_name'], **log_dict}\n",
    "                    log_dict = {k: f'{v:.5g}' if isinstance(v, float) else v for k,v in log_dict.items()}\n",
    "                    accelerator.print(f\"{prefix}[E={epoch}/{args['n_epochs']}, S={global_step}] {log_dict}\")\n",
    "\n",
    "                # 保持记录数量\n",
    "                for k, v in epoch_result_dict.items():\n",
    "                    if len(v) > 1:\n",
    "                        epoch_result_dict[k] = v[-1:]\n",
    "            accelerator.print(f\"Rank {accelerator.process_index} 训练批次 {idx} 成功\")\n",
    "        except Exception as e:\n",
    "            accelerator.print(f\"[进程 {accelerator.process_index}] 批次 {idx} 出错: {e}\")\n",
    "            accelerator.print(traceback.format_exc())\n",
    "            continue\n",
    "    # Metric summary:\n",
    "    epoch_result_dict = {k:(sum(v)/len(v) if isinstance(v, list) else v) for k, v in epoch_result_dict.items()}\n",
    "    # 更新进度条\n",
    "    if accelerator.is_main_process:\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    accelerator.print(f\"Rank {accelerator.process_index} 训练epoch {epoch} 结束\")\n",
    "    return epoch_result_dict, global_step, evaluate_result_dict\n",
    "\n",
    "def evaluate_generation(args, model, dataset, dataloader, tokenizer):\n",
    "    return {'value_accuracy': 0}\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for idx, batch in tqdm(\n",
    "        enumerate(dataloader), total=len(dataloader), disable=not accelerator.is_main_process,\n",
    "        desc='Evaluation Gen Loop'):\n",
    "        output_ = accelerator.unwrap_model(model).generate(\n",
    "            **batch['generate_prefix_kwargs'],\n",
    "            max_length=args['max_gen_length'],\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "            num_beams=1,\n",
    "            use_cache=True,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        generated_ids = output_.sequences\n",
    "        generated_ids = pad_across_processes(generated_ids, dim=1, pad_index=tokenizer.pad_token_id, pad_first=True)\n",
    "\n",
    "        labels = batch['generate_prefix_kwargs']['labels']\n",
    "        labels = pad_across_processes(labels, dim=1, pad_index=tokenizer.pad_token_id, pad_first=True)\n",
    "        labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "        generated_ids, labels = accelerator.gather(generated_ids), accelerator.gather(labels)\n",
    "\n",
    "        preds = [\n",
    "            tokenizer.decode(g.cpu().numpy().tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=True).strip() for g in\n",
    "            generated_ids]\n",
    "        predictions.extend(preds)\n",
    "        target = [\n",
    "            tokenizer.decode(t.cpu().numpy().tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=True).strip() for t in\n",
    "            labels]\n",
    "        targets.extend(target)\n",
    "\n",
    "    predictions = predictions[:len(dataset)]\n",
    "    targets = targets[:len(dataset)]\n",
    "\n",
    "    if accelerator.is_main_process and accelerator.is_local_main_process:\n",
    "        results = []\n",
    "        src_name = dataset[0]['item_id'].split('_')[0]\n",
    "        for pred, tar, item in zip(predictions, targets, dataset):\n",
    "            cur_res = {\n",
    "                'item_id': item['item_id'],\n",
    "                'answer_value': item['answer_value'],\n",
    "            }\n",
    "            ## Processing target\n",
    "            target_cot = tar.strip().split(cot_trigger)[-1].strip()\n",
    "            target_value = post_process_final_answer_fn_mapper[src_name](cur_res['answer_value'])\n",
    "            cur_res['target'] = target\n",
    "            cur_res['target_cot'] = target_cot\n",
    "            cur_res['target_value'] = target_value\n",
    "            ## Processing prediction\n",
    "            prediction_cot = pred.strip().split(cot_trigger)[-1].strip()\n",
    "            cur_res['prediction'] = pred\n",
    "            cur_res['prediction_cot'] = prediction_cot\n",
    "            cur_res['prediction_value'] = None # Tobe filled\n",
    "            results.append(cur_res)\n",
    "        print(\"=\"*100)\n",
    "        print(\"eval results:\")\n",
    "        print(results)\n",
    "        execute_fn = post_process_answer_cot_fn_mapper[(args['engine'], src_name)]\n",
    "        corr_value = 0\n",
    "        for i, prediction_value in enumerate(execute_fn([item['prediction_cot'] for item in results])):\n",
    "            target_value = results[i]['target_value']\n",
    "            is_correct = compare_answer_fn_mapper[src_name](prediction_value, target_value) if prediction_value is not None else False\n",
    "            results[i]['prediction_value'] = prediction_value\n",
    "            results[i]['is_correct'] = is_correct\n",
    "            corr_value += is_correct\n",
    "\n",
    "        res_path = args['model_dir'].rstrip('/')+ '/' + '_res.json'\n",
    "        with open(res_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        # if args['wandb_log']:\n",
    "        #     table = wandb.Table(dataframe=pd.DataFrame(results))\n",
    "        #     wandb.log({\"predictions\": table})\n",
    "\n",
    "        value_accuracy = corr_value / len(results) * 100\n",
    "        accelerator.print(f\"[Eval Info] value_accuracy: {value_accuracy:.5g}%\")\n",
    "        value_accuracy = torch.FloatTensor([value_accuracy]).to(accelerator.device)\n",
    "    else:\n",
    "        value_accuracy = torch.FloatTensor([-1.0]).to(accelerator.device)\n",
    "    value_accuracy = broadcast(value_accuracy).cpu().numpy().tolist()[0]\n",
    "\n",
    "    # Metric summary:\n",
    "    model.train()\n",
    "    return {'value_accuracy': value_accuracy}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(args):\n",
    "    accelerator = Accelerator()\n",
    "    set_seed(args['seed'] + accelerator.process_index)\n",
    "    if torch.distributed.get_rank() == 0 and args['wandb_log']:\n",
    "        wandb.init(project=args['wandb_project'], name=args['wandb_run_name'])\n",
    "        wandb.config.update(args)\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args['tokenizer_name_or_path'],\n",
    "        trust_remote_code=True,\n",
    "        padding_side='left',  # ChatGLM 使用左侧填充\n",
    "        eos_token='<|endoftext|>',  # 设置 EOS token\n",
    "        pad_token='<|endoftext|>',  # 设置 PAD token\n",
    "    )\n",
    "    \n",
    "    # 确保 tokenizer 有必要的特殊 token\n",
    "    special_tokens_dict = {\n",
    "        'pad_token': '<|endoftext|>',\n",
    "        'eos_token': '<|endoftext|>',\n",
    "        'bos_token': '<|startoftext|>',\n",
    "    }\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    (train_dataset, train_dataloader), (test_dataset, test_dataloader) = prepare_datasets_and_data_loaders(args, tokenizer)\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args['model_name_or_path'],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 添加缺失的配置\n",
    "    config._attn_implementation = \"eager\"  # 添加注意力实现方式\n",
    "    \n",
    "    # 加载模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args['model_name_or_path'],\n",
    "        config=config,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # 确保模型参数是 bf16 类型\n",
    "    model = model.bfloat16()\n",
    "    \n",
    "    accelerator.print(f'[Vocab size]: {len(tokenizer)}')    \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if accelerator.is_main_process and args['wandb_log']:\n",
    "        wandb.run.summary.update({\n",
    "            'pad_token_id': tokenizer.pad_token_id,\n",
    "            'eos_token_id': tokenizer.eos_token_id,\n",
    "            'unk_token_id': tokenizer.unk_token_id,\n",
    "            'vocab_size': len(tokenizer)\n",
    "        })\n",
    "\n",
    "    n_epochs = args['n_epochs']\n",
    "    num_training_steps = (len(train_dataloader) // accelerator.num_processes * n_epochs) // args['gradient_accumulation_steps']\n",
    "    warmup_step = args['warmup_step'] if args['warmup_step'] is not None and args['warmup_step'] >= 0 else int(0.1 * num_training_steps)\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\n",
    "            \"weight_decay\": args['weight_decay'],\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=1e-8)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=num_training_steps)\n",
    "    # scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step)\n",
    "    accelerator.print(\n",
    "        f\"***** Running training *****\\n\"\n",
    "        f\"  Num examples = {len(train_dataset)}\\n\"\n",
    "        f\"  Num Epochs = {n_epochs}\\n\"\n",
    "        f\"  Instantaneous batch size per device = {args['batch_size']}\\n\"\n",
    "        f\"  Total train batch size (w. parallel, distributed & accumulation) = {args['batch_size']*accelerator.num_processes*args['gradient_accumulation_steps']}\\n\"\n",
    "        f\"  Total optimization steps = {num_training_steps}\\n\"\n",
    "        f\"  Warm up step: {warmup_step}\\n\"\n",
    "        f\"  Learning rate: {args['learning_rate']}\\n\"\n",
    "    )   \n",
    "    model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(model, optimizer, train_dataloader, test_dataloader)\n",
    "    \n",
    "    global_step = 0\n",
    "    evaluating_epoch_freq = args['evaluating_epoch_freq']\n",
    "    logging_epoch_freq = args['logging_epoch_freq']\n",
    "    saving_epoch_freq = args['saving_epoch_freq']\n",
    "    model_dir=args['model_dir']\n",
    "    best_eval_log_dict = {}\n",
    "    summary_log_dict = {}\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    most_recent_ckpts_paths = []\n",
    "    lowest_loss = None\n",
    "    with tqdm(range(1, n_epochs+1), total=n_epochs, disable=not accelerator.is_main_process) as t:\n",
    "        for epoch in t:\n",
    "            kwargs = {\n",
    "                'args': args,\n",
    "                'model': model, \n",
    "                'train_dataset': train_dataset, \n",
    "                'train_dataloader': train_dataloader, \n",
    "                'test_dataset': test_dataset,\n",
    "                'test_dataloader': test_dataloader,\n",
    "                'optimizer': optimizer, \n",
    "                'scheduler': scheduler,\n",
    "                'global_step': global_step, \n",
    "                'tokenizer': tokenizer,\n",
    "                'prefix':'', \n",
    "                'epoch': epoch,\n",
    "                'best_eval_log_dict': best_eval_log_dict,\n",
    "                'summary_log_dict': summary_log_dict,\n",
    "                'most_recent_ckpts_paths': most_recent_ckpts_paths,\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                train_epoch_result_dict, global_step, evaluate_result_dict = train_one_epoch(**kwargs)\n",
    "                accelerator.print(f\"[进程 {accelerator.process_index}] Epoch {epoch} 训练结果: {train_epoch_result_dict}\")\n",
    " \n",
    "            except Exception as e:\n",
    "                accelerator.print(f\"[进程 {accelerator.process_index}] Epoch {epoch} 发生错误: {e}\")\n",
    "                break\n",
    "            \n",
    "            eval_log_dict = {}\n",
    "            is_best = False\n",
    "            \n",
    "            accelerator.print(f'跳过evaluation')\n",
    "            # if evaluating_epoch_freq is not None and epoch % evaluating_epoch_freq == 0:\n",
    "            #     evaluate_result_dict = {f'Eval.Gen.{k}':  v for k, v in evaluate_generation(args, model, test_dataset, test_dataloader, tokenizer).items()}\n",
    "            #     eval_log_dict.update(evaluate_result_dict)\n",
    "            #     if eval_log_dict['Eval.Gen.value_accuracy'] > best_eval_log_dict.get('Eval.Gen.value_accuracy_best', 0):\n",
    "            #         is_best = True\n",
    "            #         best_eval_log_dict['Eval.Gen.value_accuracy_best'] = eval_log_dict['Eval.Gen.value_accuracy']\n",
    "            #     if 'Eval.Gen.value_accuracy' not in summary_log_dict:\n",
    "            #         summary_log_dict['Eval.Gen.value_accuracy'] = []\n",
    "            #     summary_log_dict['Eval.Gen.value_accuracy'].append(eval_log_dict['Eval.Gen.value_accuracy'])\n",
    "            if lowest_loss is None:\n",
    "                lowest_loss = train_epoch_result_dict[\"loss\"]\n",
    "                is_best = True\n",
    "            elif train_epoch_result_dict[\"loss\"] < lowest_loss:\n",
    "                lowest_loss = train_epoch_result_dict[\"loss\"]\n",
    "                is_best = True\n",
    "                \n",
    "            train_log_dict = {}\n",
    "            if logging_epoch_freq is not None and epoch % logging_epoch_freq == 0:\n",
    "                train_log_dict = {f'T.{k}': sum(v)/len(v) if isinstance(v, list) else v for k, v in train_epoch_result_dict.items()}\n",
    "\n",
    "            if eval_log_dict or train_log_dict:\n",
    "                log_dict = {'lr': scheduler.get_last_lr()[0], **train_log_dict, **eval_log_dict, **best_eval_log_dict}\n",
    "                if accelerator.is_main_process and args['wandb_log']:\n",
    "                    wandb.log(log_dict, step=global_step)\n",
    "                    log_dict = {'wandb': args['wandb_project'] + '|' + args['wandb_run_name'], **log_dict}\n",
    "                log_dict = {k: f'{v:.5g}' if isinstance(v, float) else v for k,v in log_dict.items()}\n",
    "                accelerator.print(f\"[E={epoch}/{args['n_epochs']}, S={global_step}] {log_dict}\")\n",
    "            \n",
    "            # if saving_epoch_freq is not None and epoch % saving_epoch_freq == 0:\n",
    "            if is_best:\n",
    "                try:\n",
    "                    accelerator.wait_for_everyone()\n",
    "                    accelerator.print(f\"epoch {epoch} 将开始保存权重，等待所有进程成功\")\n",
    "                except Exception as e:\n",
    "                    accelerator.print(f\"epoch {epoch} 将开始保存权重，等待所有进程失败: {e}\")\n",
    "                    break\n",
    "                \n",
    "                save_path = model_dir\n",
    "                # 如果目录已存在,先清空\n",
    "                if accelerator.is_main_process and os.path.exists(save_path):\n",
    "                    accelerator.print(f\"目录已存在, 清空目录: {save_path}\")\n",
    "                    # shutil.rmtree(save_path)\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                accelerator.print(f\"开始保存新的最佳checkpoint... 时间: {timestamp}\")\n",
    "                \n",
    "                # accelerator.wait_for_everyone()\n",
    "                s=do_checkpoint(args, model, tokenizer, save_path, global_step)\n",
    "                # accelerator.wait_for_everyone()\n",
    "                if s:\n",
    "                    accelerator.print(f\"保存checkpoint成功\")\n",
    "                else:\n",
    "                    accelerator.print(f\"保存checkpoint失败\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    from transformers import HfArgumentParser\n",
    "    NONE_INT = -100 \n",
    "    NONE_STR = 'None'\n",
    "    @dataclass\n",
    "    class Arguments:\n",
    "        model_name_or_path: str\n",
    "        tokenizer_name_or_path: str\n",
    "        model_dir: str\n",
    "        train_file: str \n",
    "        test_file: str\n",
    "        batch_size: int = field(default=4)\n",
    "        eval_batch_size: int = field(default=8)\n",
    "        n_epochs: int = field(default=40)\n",
    "        num_workers: int = field(default=8)\n",
    "        learning_rate: float = field(default=2e-5)\n",
    "        weight_decay: float = field(default=1e-6)\n",
    "        warmup_step: int = field(default=0)\n",
    "        clip_grad_norm: float = field(default=1)\n",
    "        evaluating_epoch_freq: int = field(default=1)\n",
    "        logging_epoch_freq: int = field(default=1)\n",
    "        saving_epoch_freq: int = field(default=1000)\n",
    "        evaluating_step_freq: int = field(default=NONE_INT)\n",
    "        logging_step_freq: int = field(default=NONE_INT)\n",
    "        saving_step_freq: int = field(default=NONE_INT)\n",
    "        seed: int = field(default=42)\n",
    "        max_input_length: int = field(default=700)\n",
    "        max_gen_length: int = field(default=512)\n",
    "        gradient_accumulation_steps: int = field(default=1)\n",
    "        keep_num_ckpt: int = field(default=1)\n",
    "        # wandb stuff\n",
    "        wandb_log: bool = field(default=False)\n",
    "        wandb_project: str = field(default='tmp_anvfupsadfn')\n",
    "        wandb_run_name: str = field(default='default_run_name')\n",
    "        ###\n",
    "        engine: str = field(default='nl')\n",
    "\n",
    "    parser = HfArgumentParser(Arguments)\n",
    "    (args,) = parser.parse_args_into_dataclasses()\n",
    "    args = asdict(args)\n",
    "    for k,v in args.items():\n",
    "        if v in [NONE_INT, NONE_STR]:\n",
    "            args[k] = None\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=args['gradient_accumulation_steps'], kwargs_handlers=[InitProcessGroupKwargs(timeout=timedelta(seconds=5*60*60))]) # wait for processing upto 5hrs\n",
    "    accelerator.print(args)\n",
    "    accelerator.print(json.dumps(args, indent=2, ensure_ascii=False))\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(training_loop, args, num_processes=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
